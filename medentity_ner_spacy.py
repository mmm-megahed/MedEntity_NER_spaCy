# -*- coding: utf-8 -*-
"""MedEntity_NER_spaCy.ipynb

Automatically generated by Colaboratory.




**Script 1: Read the 97 Artikels from .txt files**
"""

import os

def read_first_20_lines(txt_directory):
    docs = []
    txt_files = [file for file in os.listdir(txt_directory) if file.endswith('.txt')]
    for txt_file in txt_files:
        txt_path = os.path.join(txt_directory, txt_file)
        with open(txt_path, 'r') as f:
            first_20_lines = [next(f) for _ in range(20)]
            text = ''.join(first_20_lines)
            docs.append({txt_file: text})
    return docs

# Example usage:
txt_directory = '/content/drive/MyDrive/CIE/CRAFT_txt'
docs = read_first_20_lines(txt_directory)
print(docs)

print(docs[2])

"""**Script 2: Parse .xml files and extract annotations**"""

import xml.etree.ElementTree as ET
import os

# Step 1: Parse XML Files and Extract Annotations
def parse_xml_files(xml_files):
    annotations = []
    for xml_file in xml_files:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        text_source = root.attrib.get('textSource')
        parent_dirs = os.path.dirname(xml_file).split(os.path.sep)[-2:-1]
        file_name = '_'.join(parent_dirs)
        for annotation in root.findall('annotation'):
            mention_id = annotation.find('mention').attrib.get('id')
            span_start = int(annotation.find('span').attrib.get('start'))
            span_end = int(annotation.find('span').attrib.get('end'))
            spanned_text = annotation.find('spannedText').text
            annotations.append({
                'text_source': text_source,
                'mention_id': mention_id,
                'span_start': span_start,
                'span_end': span_end,
                'spanned_text': spanned_text,
                'file_name': file_name  # Use the extracted directory name
            })
    return annotations

# Step 2: Specify XML Files Directories
xml_files_directories = [
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/CHEBI/CHEBI/knowtator',
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/CL/CL/knowtator',
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/GO_BP/GO_BP/knowtator',
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/GO_CC/GO_CC/knowtator',
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/GO_MF/GO_MF/knowtator',
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/MONDO/MONDO/knowtator',
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/MOP/MOP/knowtator',
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/NCBITAXON/NCBITAXON/knowtator',
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/PR/PR/knowtator',
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/SO/SO/knowtator',
    '/content/drive/MyDrive/CIE/concept-annotation (Kopie)/UBERON/UBERON/knowtator'
]

# Initialize a list to store all annotations
all_annotations = []

# Step 3: Parse XML Files and Extract Annotations for each directory
for xml_files_directory in xml_files_directories:
    # Get list of XML files in directory
    xml_files = [os.path.join(xml_files_directory, file) for file in os.listdir(xml_files_directory) if file.endswith('.xml')]
    # Parse XML Files and Extract Annotations
    annotations = parse_xml_files(xml_files)
    # Extend the list of annotations
    all_annotations.extend(annotations)

# Display all annotations
print(all_annotations[0])

len(all_annotations)

all_annotations[77]

"""**Script 3: Combine functionality of Script 1 and Script 2 spacy data Format**"""

import os
import xml.etree.ElementTree as ET

def read_first_20_chars(txt_directory):
    docs = []
    txt_files = [file for file in os.listdir(txt_directory) if file.endswith('.txt')]
    for txt_file in txt_files:
        txt_path = os.path.join(txt_directory, txt_file)
        with open(txt_path, 'r') as f:
            first_20_chars = f.read()[:20]
            docs.append({'text': first_20_chars})
    return docs

def parse_xml_files(xml_directory):
    annotations = []
    for root, dirs, files in os.walk(xml_directory):
        for file in files:
            if file.endswith('.xml'):
                xml_file = os.path.join(root, file)
                tree = ET.parse(xml_file)
                root = tree.getroot()
                text_source = root.attrib.get('textSource')
                for annotation in root.findall('annotation'):
                    mention_id = annotation.find('mention').attrib.get('id')
                    span_start = int(annotation.find('span').attrib.get('start'))
                    span_end = int(annotation.find('span').attrib.get('end'))
                    spanned_text = annotation.find('spannedText').text
                    annotations.append({
                        'text_source': text_source,
                        'mention_id': mention_id,
                        'span_start': span_start,
                        'span_end': span_end,
                        'spanned_text': spanned_text,
                        'file_name': os.path.basename(xml_file)[:-4]
                    })
    return annotations

# Example usage:
txt_directory = '/content/drive/MyDrive/CIE/CRAFT_txt'
xml_directory = '/content/drive/MyDrive/CIE/concept-annotation (Kopie)'

docs = read_first_20_chars(txt_directory)
annotations = parse_xml_files(xml_directory)

print(docs)
print(annotations)

training_data = []

for doc_info in docs:
    doc_name, doc_text = list(doc_info.items())[0]
    entities = []
    for annotation in all_annotations:
        if annotation['text_source'] == doc_name:
            entity = (
                annotation['span_start'],
                annotation['span_end'],
                annotation['file_name']
            )
            entities.append(entity)
    training_data.append({'text': doc_text, 'entities': entities})

# Print the first training example to verify the structure
print(training_data[0])

"""**Using spaCy**"""

import spacy

# !python -m spacy download en_core_web_lm
!python -m spacy download en_core_web_lg

nlp = spacy.load("en_core_web_lg")
nlp

from spacy.tokens import DocBin
from tqdm import tqdm

nlp = spacy.blank("en")
doc_bin = DocBin()

from spacy.util import filter_spans

for training_example  in tqdm(training_data):
    text = training_example['text']
    labels = training_example['entities']
    doc = nlp.make_doc(text)
    ents = []
    for start, end, label in labels:
        span = doc.char_span(start, end, label=label, alignment_mode="contract")
        if span is None:
            print("Skipping entity")
        else:
            ents.append(span)
    filtered_ents = filter_spans(ents)
    doc.ents = filtered_ents
    doc_bin.add(doc)

doc_bin.to_disk("train.spacy")

!python -m spacy init fill-config base_config.cfg config.cfg

!python -m spacy train config.cfg --output ./ --paths.train ./train.spacy --paths.dev ./train.spacy

nlp_ner = spacy.load("model-best")

doc = nlp_ner("##############'") #test

colors = colors = {
        "CHEBI": "#F67DE3",
        "CL": "#7DF6D9",
        "MONDO": "#a6e22d",
        "GO_BP": "#FF5733",
        "GO_CC": "#33FF57",
        "GO_MF": "#5733FF",
        "MOP": "#33A1FF",
        "NCBITAXON": "#A133FF",
        "PR": "#FFD033",
        "SO": "#33FFA1",
        "UBERON": "#FF23A1"}
options = {"colors": colors}

spacy.displacy.render(doc, style="ent", options= options, jupyter=True)
